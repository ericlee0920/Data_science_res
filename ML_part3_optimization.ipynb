{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Machine Learning\n",
    "### Part. 3 Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "- **Linear Regression** makes predictions using a linear function: $\\hat y_{i} = wx_{i}$.\n",
    "    - $w$ is the weight or regression coefficient of $x_{i}$, affecting rate of $\\hat y_{i}$ increase/decrease.\n",
    "- Minimize sum of squares errors objective: $f(w)=\\sum_{i=1}^{n}(wx_{i}-y_{i})^2$\n",
    "- Minimize differentiable function: solve $f'(w)=0$, choose smallest one and check $f''(w)>0$.\n",
    "    - For w of sum of squared errors: $w=\\frac{\\sum_{i}^{n}x_{i}y_{i}}{\\sum_{i}^{n}x_{i}^{2}}$. Check minimizer by checking $f''(w)=\\sum_{i}^{n}x_{i}^{2}>0$.\n",
    "- **d-Dimensional Linear Model**: $\\hat y_{i} = w_{1}x_{i1} + w_{2}x_{i2}...w_{d}x_{id} = \\sum_{j=1}^{d}w_{j}x_{ij} = w^{T}x_{i}$\n",
    "    - thus to minimize sum of squares errors objective in d-Dimensions: $f(w)=\\sum_{i=1}^{n}(w^{T}x_{i}-y_{i})^2$\n",
    "- Gradient is vector with partial derivative j in position j.\n",
    "    - gradient for sum of squares is $Aw-b+0=X^{T}Xw-X^{T}y$, the minimizer to this objective are the solutions to the linear system.\n",
    "    - forming vector $X^{T}y$ is $O(nd)$, matrix $X^{T}X$ is $O(nd^{2})$, solving d by d system of equations is $O(d^{3})$, thus overall $O(nd^{2}+d^{3})$\n",
    "- Least Squares assumes linearity. It may predict outside of range values, data can be big that we can't store $X^{T}X$, features must all be used. Solution can be not unique and it is sensitive to outliers.\n",
    "    - Solution is unique iff all columns of $X$ are linearly independent.\n",
    "- Y-intercept is omitted, but we add a bias variable that is always 1.\n",
    "- Non-linear features can be transformed by change of basis, e.g. $\\hat y_{i} = w_{1}x_{i} + w_{2}x_{i}^{2}...w_{p}x_{i}^{p}$\n",
    "    - when polynomial degree increases, training error falls, approximation error goes up when overfitting with large $p$. Use cross-validation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "- Optimization is maximizing or minimizing an objective by a minimizer.\n",
    "- **Gradient Descent** is an iterative optimization algorithm.\n",
    "    - Initialize at guess $w^{0}$\n",
    "    - Use $\\nabla f(w^{0})$ to guess $w^{1}$\n",
    "    - Use $\\nabla f(w^{1})$ to guess $w^{2}$\n",
    "    - $\\lim_{t\\to\\infty} \\nabla f(w^{t})=0$, convergence.\n",
    "- New guess: $w^{t+1}=w^{t}-\\alpha^{t} \\nabla f(w^{t})$ for t = 1,2,3...\n",
    "- For Least squares using gradient descent, cost is $O(ndt)$.\n",
    "- **Convexity** applies if area above the function is a convex set, such as:\n",
    "    - Twice-differentiable function positive for all $w$\n",
    "    - Convex function multiplied by constant\n",
    "    - Composition of convex and linear functions\n",
    "    - Sum or max of convex functions\n",
    "    - Norms or squared norms\n",
    "    - *NOT* convex: multiplication or composition of convex functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust Regression\n",
    "- In least sqaures, squaring the rror shrinks the small errors but magnifies large ones. Large errors influence $w$ much more than other points.\n",
    "- **Robust Regression** objectives focus less on large errors, e.g. use absolute error instead of squared error.\n",
    "    - L1-norm: $f(w)=||Xw-y||_{1}=\\sum_{i=1}^{n}|w^{T}x_{i}-y_{i}|$\n",
    "    - Yet, minimizing absolute error is harder since we don't have normal equations, absolute value is non-differentiable at 0.\n",
    "    - Unlike smooth functions, gradient may not get smaller near a minimizer. To apply gradient descent, we use smooth approximation.\n",
    "    - Huber Loss is a smooth approximations to absolute value: $f(w)=\\sum_{i=1}^{n}h(w^{T}x_{i}-y_{i})$\n",
    "     $ h(r_{i})=   \\left\\{\n",
    "\\begin{array}{ll}\n",
    "      \\frac{1}{2}r_{i}^{2} & |r_{i}|<=\\epsilon \\\\\n",
    "      \\epsilon(|r_{i}|-\\frac{1}{2}\\epsilon) & otherwise \\\\\n",
    "\\end{array} \n",
    "\\right.$\n",
    "    - \"h\" is differentiable, $h'(\\epsilon)=\\epsilon$\n",
    "    - \"f\" is convex but $\\nabla f(x)=0$ doesn't give a linear system. We can minimize Huber loss using gradient descent.\n",
    "- Non-convex errors can be very robust, not influenced by outliers, but finding global minimum is hard.\n",
    "- Brittle Regression: using $L_{\\infty}$ error is more sensitive to outliers. Also not differentiable, use logsumexp to solve.\n",
    "- Log-Sum-Exp: $max_{i}(z_{i})\\approx \\ln(\\sum_{i}exp(z_{i}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the True Model\n",
    "- There are lot of scores used to find the true model. \n",
    "- The idea is to put a penalty on the model complexity, e.g. minimize training error plus the degree of polynomial $score(p)=\\frac{1}{2}||Z_{p}v-y||^{2}+p$. We optimize by:\n",
    "    - form $Z_{0}$, solve for v, compute score(0)\n",
    "    - form $Z_{1}$, solve for v, compute score(1)...\n",
    "    - choose degree with lowest score\n",
    "- Information Criteria are in the form of $score(p)=\\frac{1}{2}||Z_{p}v-y||^{2}+\\lambda k$, where $k$ is the degrees of freedom, and $\\lambda > 0$ controls how strong we penalize complexity (you need to decrease training error by at least $\\lambda$ to increase $k$ by 1).\n",
    "- $\\lambda =1$ is called Akaike information criterion (AIC), others include Mallows $C_{p}$, ajusted $R^{2}$, ANOVA-based model selection, Bayesian IC where $\\lambda =\\frac{1}{2}log(n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "- Feature is relevant if helps predict $y_{i}$ from $x_{i}$.\n",
    "- Association Approach: for each feature $j$, compute correlation between feature values $x^{j}$ and $y$. This turns feature selection into hypothesis testing for each feature and ignores variable interctions.\n",
    "    - GWAS: measure association between each single nucleotide polymorphism in the genome and a disease. Yet, huge number of false positives.\n",
    "- Regression Weight Approach: fit regression weights based on all features, take all features $j$ where weight $|w_{j}|$ is greater than a threshold. This has major problems with **collinearity**.\n",
    "- **Search and Score** Method: define score function $f(S)$ that measures quality of a set of features $S$.\n",
    "    - compute score of feature {1}\n",
    "    - compute score of feature {2}\n",
    "    - compute score of feature {1,2}\n",
    "    - compute score of feature {}\n",
    "    - return set of features with best score\n",
    "- Score can be the validation error. If we have $d$ variables, there will be $2^{d}$ sets of variables. Prone to false positives and optimization bias is high.\n",
    "    - To reduce false positives, we can use complexity penalties: $score(S)=\\frac{1}{2}\\sum_{i=1}^{n}(w_{s}^{T}x_{is}-y_{i})^{2}+ size(S)$, if two $S$ has similar error, take the smaller set.\n",
    "    - instead of $size(S)$, we write L0-norm which is the number of non-zero values.\n",
    "- Score can be the **L0-Norm Penalty / L0-Regularization**: $f(w)=\\frac{1}{2}||Xw-y||^{2}+ \\lambda ||w||_{0}$.\n",
    "- Search and Score is hard to search for best $S$ due to $2^{d}$ sets. Solve by using Forward Selection which is a greedy search heuristic $O(d^{2})$.\n",
    "    - compute score if use no features {}\n",
    "    - add each feature once and add feature with the best score to {}\n",
    "    - add each feature to the {feature_1} once and add feature with the best score, check if {feature_1 U feature_2} improves the best score, then iterate.\n",
    "- Relevance in feature selection depends on collinearity, conditional independence, effect size, confounding, causality, context-specific problems. Relevance doesn't imply causal relationship.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2-Regularization / Ridge Regression\n",
    "- Regularization adds a penalty on the complexity of the model. This increases training error but decreases approximation error.\n",
    "- L2-regularization: $f(w)=\\frac{1}{2}||Xw-y||^{2}+ \\frac{\\lambda}{2} ||w||^{2}$.\n",
    "    - intuition is large slopes $w_{j}$ tend to lead to overfitting. Larger $\\lambda$ puts larger penalty on slopes.\n",
    "    - How to choose $\\lambda$? as n grows $\\lambda$ should be in the range $O(1)->O(\\sqrt{n})$.\n",
    "- Regularization path is a plot of optimal weights $w_{j}$ as $lambda$ varies.\n",
    "- L2 gradient: $\\nabla f(w)=X^{T}Xw-X^{T}y+\\lambda w$\n",
    "- L2 linear system: $(X^{T}X+\\lambda I)w=X^{T}y$\n",
    "    - unlike $X^{T}X$, matrix $(X^{T}X+\\lambda I)$ is always invertible.\n",
    "    - multiple by its inverse for unique solution.\n",
    "- L2 Gradient Descent Iterations: $w^{t+1}=w^{t}-\\alpha^{t}\\nabla f(w^{t})$ $\\nabla f(w^{t})=X^{T}(Xw^{t}-y)+\\lambda w$, number of iterations decrease as $\\lambda$ increases.\n",
    "- Should we standardize unit? \n",
    "    - It doesn't matter for decision trees, naive Bayes, least squares.\n",
    "    - It does for KNN, regularized least squares.\n",
    "- Standardizing continuous features using z-scores for each feature is common practice. Standardizing test data should use mean and standard deviation of the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Radial Basis Function\n",
    "- Parametric Bases: linear models with polynomials, exponentials, logs, trignometric functions.\n",
    "- Non-parametric Bases: size of basis grows with $n$, with enough data can model complicated functions, e.g. Gaussian RBF.\n",
    "- Gaussian RBF: universal approximators that acheive optimal test error as n goes to infinity.\n",
    "    - intuition is to use $n$ bumps where each bump is centered on one training example $x_{i}$. Fitting regression weights $w$ gives us heights of bumps. Widths of bumps is a hyper-parameter, the more narrow the more complicated the model.\n",
    "- Radial Basis Functions (RBF): a set of non-parametric bases that depend on distances to training points. $replace\\ x_{i}=(x_{i1},x_{i2}...x_{id})\\ with\\ z_{i}=(g(||x_{i}-x_{1}||),g(||x_{i}-x_{2}||)...g(||x_{i}-x_{n}||)) $\n",
    "    - going from $d$ features to $n$ features.\n",
    "    - having $n$ features with each feature $j$ depending on distance to example $i$.\n",
    "    - Gaussian RBF is $g$: $g(\\epsilon)=\\exp{(-\\frac{\\epsilon^{2}}{2\\sigma^{2}})}$, variance here is a hyper-parameter controlling width.\n",
    "    - Constructing RBF given data $X$ and hyper-parameter $\\sigma$.\n",
    "        - Z = np.zeros((n,n))\n",
    "        - for i in range(n) and for j in range(n):\n",
    "            - $Z[i,j]=np.exp(\\frac{-norm(X[i,:]-X[j,:])^{2}}{2\\sigma^{2}})$\n",
    "- Gaussian RBF predictions: $\\hat{y_{i}}=\\sum_{j=1}^{n}w_{j}\\exp{(-\\frac{||x_{i}-x_{j}||^{2}}{2\\sigma^{2}})}$\n",
    "    - flexible bases can model any continuous function, but with $n$ data points RBF has $n$ basis functions.\n",
    "    - avoid overfitting by regularizing $w$ and using validation error to choose $\\sigma$ and $\\lambda$.\n",
    "- Good Model Example: RBF basis with L2-regularization and cross-validation to choose $\\sigma$ and $\\lambda$.\n",
    "    - for each value of $\\sigma$ and $\\lambda$:\n",
    "        - compute $Z$ on training data\n",
    "        - compute best $v$, $v=(Z^{T}Z+\\lambda I)^{-1}Z^{T}y$\n",
    "        - compute $\\tilde{Z}$ on validation data using training data distances\n",
    "        - make predictions $\\hat{y}=\\tilde{Z}v$\n",
    "        - compute validation error $||\\hat{y}-\\tilde{y}||^{2}$\n",
    "- Hyper-parameter Optimization: try to efficiently find best parameters, e.g. coordinate search, stochastic local search, Bayesian optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-Regularization / LASSO Regularization\n",
    "- L1-regularization: $f(w)=\\frac{1}{2}||Xw-y||^{2}+ \\lambda ||w||_{1}$.\n",
    "    - convex and improves test error, like L2-norm\n",
    "    - encourages elements of $w$ to be 0, like L0-norm\n",
    "    - simultaneously regularizes and selects features   \n",
    "    - requires iterative solver\n",
    "    - solution is not unique, unlike L2-regularization\n",
    "    - can learn with exponential number of irrelevant features, unlike L2 which learn only linear number of irrelevant features.\n",
    "    \n",
    "- Note: $f(w)=\\frac{1}{2}||Xw-y||^{2}+ \\lambda ||w||_{1}$ is L2-loss + L1-regularizer\n",
    "\n",
    "### Summary Table for Regularizers\n",
    "\n",
    "|  | sparse w | speed | unique w  | irrelevant features|\n",
    "|------|------|------|------|------|\n",
    "|L0 | Yes | Slow | No | Not Sensitive |\n",
    "|L1 | Yes | Fast | No | Not Sensitive |\n",
    "|L2 | No | Fast | Yes | Sensitive |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Classifiers\n",
    "- Perceptron Algorithm for Linear-Seperable Data\n",
    "    - start with $w_{0}=0$\n",
    "    - go through examples until you make mistake predicting $y_{i}$, set $w^{t+1}=w^{t}+y_{i}x_{i}$\n",
    "    - resume until no errors on training data\n",
    "- If perfect classifier exists, algorithms finds one in finite steps.\n",
    "- 0-1 Loss Function: you either get classification wrong(1) or right(0).\n",
    "    - ideal but non-smooth and non-convex in $w$, if no perfect classifer exists then it is a hard problem. Gradient is 0 everywhere.\n",
    "- Degenerate convex approximation to 0-1 loss\n",
    "    - if $y_{i}=+1$, we get label right if $w^{T}x_{i}>0$\n",
    "    - if $y_{i}=-1$, we get label right if $w^{T}x_{i}<0$\n",
    "    - thus, classifying $i$ correctly is $y_{i}w^{T}x_{i}>0$\n",
    "    - this has degenerate solution of $\\sum_{i=1}^{n}\\max {(0, -y_{i}w^{T}x_{i})}$ since $f(0)=0$ and the lowest possible of $f$. Use hinge or logistic loss.\n",
    "- Hinge Loss: $f(w)=\\sum_{i=1}^{n}\\max {(0, 1-y_{i}w^{T}x_{i})}$\n",
    "    - convex, not degenerate $w=0$ gives $error=1$\n",
    "- Support Vector Machine is hinge loss + L2-regularization: $f(w)=\\sum_{i=1}^{n}\\max {(0, 1-y_{i}w^{T}x_{i})} + \\frac{\\lambda}{2}||w||^{2}$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Loss: $\\sum_{i=1}^{n}\\max {(0, -y_{i}w^{T}x_{i})} \\approx \\log{(\\exp{(0)}+\\exp{(-y_{i}w^{T}x_{i}}))}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.optimize import minimize\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min = 3.0000000055708536\n",
      "arg min = [5. 5. 5.]\n"
     ]
    }
   ],
   "source": [
    "# optimization using minimization\n",
    "result = minimize(lambda x: np.linalg.norm(x-5)+3, np.random.rand(3))\n",
    "print(\"min =\", result.fun)\n",
    "print(\"arg min =\", result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = -0.005920\n",
      "b = -4.363263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def gen_outlier_data(n=40,Noutliers=3):\n",
    "    # generate random data\n",
    "    x = np.random.randn(n)\n",
    "    y = 10*x\n",
    "    # add random outliers\n",
    "    y[:Noutliers] = -100*(x[:Noutliers]+np.random.randn(Noutliers))\n",
    "    X = x[:,None]\n",
    "    return X,y\n",
    "\n",
    "# fit a linear regression model\n",
    "X,y = gen_outlier_data()\n",
    "lr = sklearn.linear_model.LinearRegression()\n",
    "lr.fit(X,y)\n",
    "\n",
    "print(\"w = %f\" % lr.coef_)\n",
    "print(\"b = %f\" % lr.intercept_)\n",
    "plt.plot(X,y,'.')\n",
    "plt.plot(X, lr.predict(X))\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = 9.996783\n",
      "β = 0.001844\n"
     ]
    }
   ],
   "source": [
    "x = X.flatten()\n",
    "w = 0\n",
    "β = 0\n",
    "α = 1\n",
    "\n",
    "Nsteps = 1000\n",
    "\n",
    "for t in range(1,Nsteps):\n",
    "    dLdw = np.sum(x*np.sign(w*x+β-y))\n",
    "    dLdβ = np.sum(np.sign(w*x+β-y))\n",
    "    w -= (α/t)*dLdw # we are decreasing the step size over time to deal with the non-smoothness\n",
    "    β -= (α/t)*dLdβ # the details are beyond the scope of the course, see CPSC 540\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(x,y,'.')\n",
    "plt.plot(x,w*x+β)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()\n",
    "\n",
    "print(\"w = %f\" % w)\n",
    "print(\"β = %f\" % β)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
